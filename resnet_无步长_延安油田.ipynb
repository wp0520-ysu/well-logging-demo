{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet 无步长 延安油田.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMu8K48ILUnh1wHqcIqaV7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunyingjian/AI-in-well-logging/blob/master/resnet_%E6%97%A0%E6%AD%A5%E9%95%BF_%E5%BB%B6%E5%AE%89%E6%B2%B9%E7%94%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYVqvdH2sZd"
      },
      "source": [
        "#引入数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndY-lE5r2mYK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9db1b0d1-f756-458a-8d19-4ba3e96325fb"
      },
      "source": [
        "!git clone https://github.com/sunyingjian/numpy-.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'numpy-' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDSYbCgLzpj4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F8HwmeszsVv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "74a01df7-8c84-4f84-9092-c25b00d79428"
      },
      "source": [
        "%matplotlib inline\n",
        "#%matplotlib inline 可以在Ipython编译器里直接使用，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步。\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.colors as colors\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from pandas import set_option\n",
        "set_option(\"display.max_rows\", 10)#设置要显示的默认行数，显示的最大行数是10\n",
        "pd.options.mode.chained_assignment = None #为了在增加列表行数的时候防止出现setting with copy warning\n",
        "training_data = pd.read_csv('/content/numpy-/3345train data.csv')\n",
        "training_data\n",
        "testing_data = pd.read_csv('/content/numpy-/3345test_data.csv')\n",
        "testing_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AC</th>\n",
              "      <th>CAL</th>\n",
              "      <th>GR</th>\n",
              "      <th>K</th>\n",
              "      <th>RD</th>\n",
              "      <th>SP</th>\n",
              "      <th>Core Lithology</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038756</td>\n",
              "      <td>0.026970</td>\n",
              "      <td>0.094776</td>\n",
              "      <td>0.112543</td>\n",
              "      <td>0.001420</td>\n",
              "      <td>0.614504</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.347382</td>\n",
              "      <td>0.090194</td>\n",
              "      <td>0.145150</td>\n",
              "      <td>0.202335</td>\n",
              "      <td>0.002744</td>\n",
              "      <td>0.339568</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.218400</td>\n",
              "      <td>0.059510</td>\n",
              "      <td>0.252165</td>\n",
              "      <td>0.711295</td>\n",
              "      <td>0.003874</td>\n",
              "      <td>0.918567</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.366064</td>\n",
              "      <td>0.082147</td>\n",
              "      <td>0.149304</td>\n",
              "      <td>0.201731</td>\n",
              "      <td>0.003290</td>\n",
              "      <td>0.343180</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.291877</td>\n",
              "      <td>0.079981</td>\n",
              "      <td>0.141782</td>\n",
              "      <td>0.289309</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>0.344605</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>0.016947</td>\n",
              "      <td>0.138827</td>\n",
              "      <td>0.118400</td>\n",
              "      <td>0.358567</td>\n",
              "      <td>0.281687</td>\n",
              "      <td>0.302936</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>0.131043</td>\n",
              "      <td>0.069016</td>\n",
              "      <td>0.029354</td>\n",
              "      <td>0.060197</td>\n",
              "      <td>0.282093</td>\n",
              "      <td>0.253809</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>0.036182</td>\n",
              "      <td>0.032010</td>\n",
              "      <td>0.030090</td>\n",
              "      <td>0.071069</td>\n",
              "      <td>0.359737</td>\n",
              "      <td>0.435486</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>0.014562</td>\n",
              "      <td>0.127332</td>\n",
              "      <td>0.035014</td>\n",
              "      <td>0.178579</td>\n",
              "      <td>0.803566</td>\n",
              "      <td>0.311641</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>0.014427</td>\n",
              "      <td>0.134185</td>\n",
              "      <td>0.033460</td>\n",
              "      <td>0.186430</td>\n",
              "      <td>0.891134</td>\n",
              "      <td>0.311652</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>366 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           AC       CAL        GR         K        RD        SP  Core Lithology\n",
              "0    0.038756  0.026970  0.094776  0.112543  0.001420  0.614504               5\n",
              "1    0.347382  0.090194  0.145150  0.202335  0.002744  0.339568               7\n",
              "2    0.218400  0.059510  0.252165  0.711295  0.003874  0.918567               3\n",
              "3    0.366064  0.082147  0.149304  0.201731  0.003290  0.343180               7\n",
              "4    0.291877  0.079981  0.141782  0.289309  0.003455  0.344605               7\n",
              "..        ...       ...       ...       ...       ...       ...             ...\n",
              "361  0.016947  0.138827  0.118400  0.358567  0.281687  0.302936               6\n",
              "362  0.131043  0.069016  0.029354  0.060197  0.282093  0.253809               3\n",
              "363  0.036182  0.032010  0.030090  0.071069  0.359737  0.435486               5\n",
              "364  0.014562  0.127332  0.035014  0.178579  0.803566  0.311641               6\n",
              "365  0.014427  0.134185  0.033460  0.186430  0.891134  0.311652               6\n",
              "\n",
              "[366 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Eb8yf4Rz28T"
      },
      "source": [
        "##smote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRh4LNHdz402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "a32729af-3b8a-42ee-f26b-379d53baead5"
      },
      "source": [
        "a = training_data['Core Lithology'].value_counts()\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7    928\n",
              "3    751\n",
              "2    710\n",
              "5    482\n",
              "1    194\n",
              "4    166\n",
              "6    114\n",
              "Name: Core Lithology, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60yadGdZz7Tk"
      },
      "source": [
        "X_train = training_data.drop(columns='Core Lithology')\n",
        "y_train = training_data['Core Lithology'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM3EcObqz9ot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "66285426-271d-4719-e7ca-1cb3362d3838"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "sm = SMOTE(sampling_strategy={5:700,1:700,4:700,6:700},random_state=24)\n",
        "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n",
        "\n",
        "train_x_sm = pd.DataFrame(X_train_sm)\n",
        "train_y_sm = pd.DataFrame(y_train_sm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ma02_Uj0BA7"
      },
      "source": [
        "train_x_sm['Core Lithology'] = y_train_sm\n",
        "training_data_sm = train_x_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utNU8AEG0RG2"
      },
      "source": [
        "X_train_sm = training_data_sm.drop(columns='Core Lithology')\n",
        "y_train_sm = training_data_sm['Core Lithology'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpPzlCx2wNgp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a7f6d33-f7c2-4fde-d4d0-7ceb257f4771"
      },
      "source": [
        "X_train_sm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5189, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ethW-ZucxMd_"
      },
      "source": [
        "#升维\n",
        "X_train_sm = tf.expand_dims(X_train_sm,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eNqgnglxfEL"
      },
      "source": [
        "train_x = X_train_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7NsvlnuxqeS"
      },
      "source": [
        "X_test = testing_data.drop(columns='Core Lithology')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-o0RoTxx2j"
      },
      "source": [
        "y_test = testing_data['Core Lithology'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0lBhoPNx3Mm"
      },
      "source": [
        "X_test = tf.expand_dims(X_test, 1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zedVH_zKzjX3"
      },
      "source": [
        "test_x = X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vhXVkkSyGis"
      },
      "source": [
        "#onehot编码\n",
        "y_test = y_test-1\n",
        "y_train_sm = y_train_sm-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrWqfNBQyVo9"
      },
      "source": [
        "train_y = tf.keras.utils.to_categorical(y_train_sm,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j593Wwrmycj8"
      },
      "source": [
        "test_y = tf.keras.utils.to_categorical(y_test,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsXRifKM0Ra9"
      },
      "source": [
        "##ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVyaGbKwA61"
      },
      "source": [
        "from tensorflow import keras\n",
        "import math\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7WZvR_lyoL9"
      },
      "source": [
        "input_layer = tf.keras.Input(shape=(1,6))\n",
        "n_feature_maps = 16\n",
        "attention = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7FlZGoIyqcX"
      },
      "source": [
        "# BLOCK 1\n",
        "\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, strides=attention,padding='same')(input_layer)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, strides=attention, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps, strides=attention, kernel_size=3, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# expand channels for the sum\n",
        "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, strides=attention, kernel_size=1, padding='same')(input_layer)\n",
        "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "# BLOCK 2\n",
        "\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, strides=attention*2, kernel_size=8, padding='same')(output_block_1)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, strides=attention*2, kernel_size=5, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, strides=attention*2, kernel_size=3, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# expand channels for the sum\n",
        "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, strides=attention*2, kernel_size=1, padding='same')(output_block_1)\n",
        "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "# BLOCK 3\n",
        "\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, strides=attention*2, kernel_size=8, padding='same')(output_block_2)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, strides=attention*2, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, strides=attention*2, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# no need to expand channels because they are equal\n",
        "shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "# FINAL\n",
        "\n",
        "gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "output_layer = keras.layers.Dense(7, activation='softmax')(gap_layer)\n",
        "\n",
        "model = keras.models.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfVfMqHlzQSD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f0ade2b-8210-43af-ce17-eae38c475bde"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1, 6)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 1, 16)        784         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 1, 16)        64          conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 1, 16)        0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 1, 16)        1296        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 1, 16)        64          conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 1, 16)        0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 1, 16)        112         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 1, 16)        784         activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 1, 16)        64          conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 1, 16)        64          conv1d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 1, 16)        0           batch_normalization_39[0][0]     \n",
            "                                                                 batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 1, 16)        0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 1, 32)        4128        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 1, 32)        128         conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 1, 32)        0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 1, 32)        5152        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 1, 32)        128         conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 1, 32)        0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 1, 32)        544         activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 1, 32)        3104        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 1, 32)        128         conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 1, 32)        128         conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 1, 32)        0           batch_normalization_43[0][0]     \n",
            "                                                                 batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 1, 32)        0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 1, 32)        8224        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 1, 32)        128         conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 1, 32)        0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_42 (Conv1D)              (None, 1, 32)        5152        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 1, 32)        128         conv1d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 1, 32)        0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_43 (Conv1D)              (None, 1, 32)        3104        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 1, 32)        128         activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 1, 32)        128         conv1d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 1, 32)        0           batch_normalization_47[0][0]     \n",
            "                                                                 batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 1, 32)        0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 32)           0           activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7)            231         global_average_pooling1d_3[0][0] \n",
            "==================================================================================================\n",
            "Total params: 33,895\n",
            "Trainable params: 33,255\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ05l_s7zTS2"
      },
      "source": [
        "#学习率衰减\n",
        "learning_rate=0.001\n",
        "def scheduler(epoch):\n",
        "  if epoch<=200:\n",
        "    learning_rate=0.001\n",
        "  elif epoch<=300:\n",
        "    learning_rate=0.0008\n",
        "  elif epoch<=400:\n",
        "    learning_rate=0.0006\n",
        "  else:\n",
        "    learning_rate=0.0005\n",
        "  return learning_rate\n",
        "  \n",
        "Lr_change=keras.callbacks.LearningRateScheduler(scheduler)\n",
        "#保存准确率最好的模型\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"/content/drive/My Drive/best_weight.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
        "\n",
        "#早停止\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "#Adam优化器\n",
        "Adam=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=Adam,loss='categorical_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9-FAuImzXjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bb3e130-5458-41a8-9631-6cb20d6488ef"
      },
      "source": [
        "history=model.fit(train_x,train_y,batch_size=128 ,\n",
        "         epochs=100,  \n",
        "         callbacks=[Lr_change,checkpoint,callback],\n",
        "         validation_data=(test_x,test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.5784 - acc: 0.4407\n",
            "Epoch 00001: val_acc improved from -inf to 0.20219, saving model to /content/drive/My Drive/best_weight.h5\n",
            "41/41 [==============================] - 1s 17ms/step - loss: 1.5784 - acc: 0.4407 - val_loss: 1.9469 - val_acc: 0.2022\n",
            "Epoch 2/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 1.0584 - acc: 0.6354\n",
            "Epoch 00002: val_acc improved from 0.20219 to 0.20492, saving model to /content/drive/My Drive/best_weight.h5\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 1.0389 - acc: 0.6433 - val_loss: 1.9773 - val_acc: 0.2049\n",
            "Epoch 3/100\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.8628 - acc: 0.7105\n",
            "Epoch 00003: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.8628 - acc: 0.7105 - val_loss: 2.0215 - val_acc: 0.1776\n",
            "Epoch 4/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.7662 - acc: 0.7331\n",
            "Epoch 00004: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.7579 - acc: 0.7358 - val_loss: 2.0988 - val_acc: 0.1885\n",
            "Epoch 5/100\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.7551\n",
            "Epoch 00005: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.6916 - acc: 0.7558 - val_loss: 2.1981 - val_acc: 0.1120\n",
            "Epoch 6/100\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.6445 - acc: 0.7748\n",
            "Epoch 00006: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.6443 - acc: 0.7745 - val_loss: 2.3560 - val_acc: 0.1011\n",
            "Epoch 7/100\n",
            "37/41 [==========================>...] - ETA: 0s - loss: 0.6044 - acc: 0.7872\n",
            "Epoch 00007: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.6073 - acc: 0.7849 - val_loss: 2.5601 - val_acc: 0.1202\n",
            "Epoch 8/100\n",
            "38/41 [==========================>...] - ETA: 0s - loss: 0.5690 - acc: 0.8014\n",
            "Epoch 00008: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.5743 - acc: 0.7978 - val_loss: 2.8208 - val_acc: 0.1038\n",
            "Epoch 9/100\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.5508 - acc: 0.7963\n",
            "Epoch 00009: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.5508 - acc: 0.7963 - val_loss: 3.1413 - val_acc: 0.1066\n",
            "Epoch 10/100\n",
            "35/41 [========================>.....] - ETA: 0s - loss: 0.5574 - acc: 0.7967\n",
            "Epoch 00010: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.5539 - acc: 0.7988 - val_loss: 3.4472 - val_acc: 0.1393\n",
            "Epoch 11/100\n",
            "37/41 [==========================>...] - ETA: 0s - loss: 0.5160 - acc: 0.8119\n",
            "Epoch 00011: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.5248 - acc: 0.8094 - val_loss: 3.7712 - val_acc: 0.1475\n",
            "Epoch 12/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.5047 - acc: 0.8060\n",
            "Epoch 00012: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.5080 - acc: 0.8046 - val_loss: 4.3679 - val_acc: 0.1366\n",
            "Epoch 13/100\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.4922 - acc: 0.8163\n",
            "Epoch 00013: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4922 - acc: 0.8163 - val_loss: 4.8452 - val_acc: 0.1066\n",
            "Epoch 14/100\n",
            "37/41 [==========================>...] - ETA: 0s - loss: 0.4621 - acc: 0.8294\n",
            "Epoch 00014: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.4729 - acc: 0.8239 - val_loss: 5.1375 - val_acc: 0.0874\n",
            "Epoch 15/100\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.4730 - acc: 0.8269\n",
            "Epoch 00015: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4730 - acc: 0.8269 - val_loss: 5.4652 - val_acc: 0.0710\n",
            "Epoch 16/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.4656 - acc: 0.8253\n",
            "Epoch 00016: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4673 - acc: 0.8256 - val_loss: 5.5317 - val_acc: 0.0492\n",
            "Epoch 17/100\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8311\n",
            "Epoch 00017: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4530 - acc: 0.8285 - val_loss: 5.5533 - val_acc: 0.0464\n",
            "Epoch 18/100\n",
            "40/41 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8355\n",
            "Epoch 00018: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 9ms/step - loss: 0.4589 - acc: 0.8339 - val_loss: 6.0526 - val_acc: 0.0601\n",
            "Epoch 19/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.4243 - acc: 0.8411\n",
            "Epoch 00019: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4287 - acc: 0.8400 - val_loss: 6.1189 - val_acc: 0.0437\n",
            "Epoch 20/100\n",
            "35/41 [========================>.....] - ETA: 0s - loss: 0.4229 - acc: 0.8449\n",
            "Epoch 00020: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4193 - acc: 0.8445 - val_loss: 6.0108 - val_acc: 0.0656\n",
            "Epoch 21/100\n",
            "36/41 [=========================>....] - ETA: 0s - loss: 0.4184 - acc: 0.8457\n",
            "Epoch 00021: val_acc did not improve from 0.20492\n",
            "41/41 [==============================] - 0s 8ms/step - loss: 0.4190 - acc: 0.8452 - val_loss: 5.9258 - val_acc: 0.0792\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}